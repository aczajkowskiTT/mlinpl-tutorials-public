{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf35b6d",
   "metadata": {},
   "source": [
    "## Running LLM inference on Tenstorrent hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c360cd64",
   "metadata": {},
   "source": [
    "If you want to run an LLM, you donâ€™t have to start from scratch! \n",
    "\n",
    "Tenstorrent provides optimized implementations of many common models, like many version of LLama and Qwen.\n",
    "\n",
    "We also maintain a fork of the inference engine vLLM, which lets you create production deployments!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0180d468",
   "metadata": {},
   "source": [
    "For a list of supported models and the required hardware, see the table [here](https://tenstorrent.com/developers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7ed82",
   "metadata": {},
   "source": [
    "In this tutorial, we will we using [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct). We will first run a demo to verify our setup, then deploy it on an inference server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4234a",
   "metadata": {},
   "source": [
    "### Jupyter terminal tip\n",
    "To get access to a terminal when accessing Jupyter server through a browser, click on the Jupyter logo to go to the dashboard, then click `New > Other > Terminal`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14a2c8",
   "metadata": {},
   "source": [
    "### Verifying access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b8e36a",
   "metadata": {},
   "source": [
    "_Llama-3.1-8B-Instruct_ is a gated model, which means you have to accept the license agreement before obtaining access.\n",
    "\n",
    "At the start of this tutorial, you requested this access. Let's verify that everything has been processed correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc5ad3",
   "metadata": {},
   "source": [
    "Make sure you are logged into your huggingface account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7aa430",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf auth whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26843fbb",
   "metadata": {},
   "source": [
    "If not, then log in. Copy the following command into your terminal, and follow the instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6bac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hf auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f308f001",
   "metadata": {},
   "source": [
    "And download the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ad6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf download meta-llama/Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8c213",
   "metadata": {},
   "source": [
    "Finally, try accessing the config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4571a",
   "metadata": {},
   "source": [
    "### Running a demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dbbf56",
   "metadata": {},
   "source": [
    "Demo scripts are included with your tt-metal installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db748de8",
   "metadata": {},
   "source": [
    "The scripts rely on model implementations from the TT-Transformers kit.\n",
    "Have a peek at the source!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aced68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 50 $TT_METAL_HOME/models/tt_transformers/tt/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02697aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_MODEL=meta-llama/Llama-3.1-8B-Instruct\n",
    "# export does not persist across cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $HF_MODEL\n",
    "!cd $TT_METAL_HOME && pytest models/tt_transformers/demo/simple_text_demo.py -k \"performance and batch-32\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f3569",
   "metadata": {},
   "source": [
    "### Building the vllm environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8be798",
   "metadata": {},
   "source": [
    "tt-metal comes pre-built in your environment, vllm does not. Start by cloning the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e46f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/tenstorrent/vllm.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc81cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd vllm && git checkout dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ccb377",
   "metadata": {},
   "source": [
    "Note that the Tenstorrent fork's main branch is `dev`. You can find the README [here](https://github.com/tenstorrent/vllm/blob/dev/tt_metal/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeeba00",
   "metadata": {},
   "source": [
    "Then build an environment with vllm and it's dependencies by running the following commands **in your terminal**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa0a740",
   "metadata": {},
   "source": [
    "```\n",
    "bash\n",
    "cd vllm\n",
    "export vllm_dir=$(pwd)\n",
    "source $vllm_dir/tt_metal/setup-metal.sh\n",
    "cd $TT_METAL_HOME\n",
    "./create_venv.sh\n",
    "cd $vllm_dir\n",
    "echo '\n",
    "# Custom environment variables for tt-metal\n",
    "export TT_METAL_HOME=\"'\"$TT_METAL_HOME\"'\"\n",
    "export PYTHONPATH=\"${TT_METAL_HOME}\"\n",
    "# HF caching\n",
    "export HF_HUB_CACHE=\"'\"$(pwd)/../hf_cache\"'\"\n",
    "export HF_XET_CACHE=\"'\"$(pwd)/../hf_xet\"'\"\n",
    "mkdir -p $HF_XET_CACHE\n",
    "mkdir -p $HF_HUB_CACHE\n",
    "export TT_CACHE_PATH=\"${TT_METAL_HOME}/../tt_cache\"\n",
    "mkdir -p $TT_CACHE_PATH\n",
    "' >> $PYTHON_ENV_DIR/bin/activate\n",
    "source $PYTHON_ENV_DIR/bin/activate\n",
    "pip3 install --upgrade pip\n",
    "cd $vllm_dir && pip install -e . --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "pip install -r $TT_METAL_HOME/models/tt_transformers/requirements.txt\n",
    "echo \"vllm environment created in $PYTHON_ENV_DIR\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fabd3d8",
   "metadata": {},
   "source": [
    "### Running an inference server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf5f254",
   "metadata": {},
   "source": [
    "### Run the following commands in **your terminal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c877b7",
   "metadata": {},
   "source": [
    "Activate the newly created environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7000974",
   "metadata": {},
   "source": [
    "```\n",
    "source $TT_METAL_HOME/build/python_env_vllm/bin/activate\n",
    "cd vllm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015436d",
   "metadata": {},
   "source": [
    "Test the vllm installation with an offline inference example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc28744a",
   "metadata": {},
   "source": [
    "```\n",
    "HF_MODEL=\"meta-llama/Llama-3.1-8B-Instruct\" python examples/offline_inference_tt.py --measure_perf --model \"meta-llama/Llama-3.1-8B-Instruct\" --max_model_len 65536\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9f9c6a",
   "metadata": {},
   "source": [
    "If everything worked, you can now start a server with an OpenAI-compatible API:\n",
    "(still in *your terminal*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f673f0",
   "metadata": {},
   "source": [
    "```\n",
    "VLLM_RPC_TIMEOUT=100000 HF_MODEL=\"meta-llama/Llama-3.1-8B-Instruct\" MESH_DEVICE=N150 python examples/server_example_tt.py --model \"meta-llama/Llama-3.1-8B-Instruct\" --max_model_len 65536 --num_scheduler_steps 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897222d5",
   "metadata": {},
   "source": [
    "You can benchmark its performance with another script:\n",
    "\n",
    "**start another terminal and enter the vllm virtual env before running**\n",
    "```\n",
    "python3 vllm/benchmarks/benchmark_serving.py \\\n",
    "            --backend vllm \\\n",
    "            --model \"meta-llama/Llama-3.1-8B-Instruct\" \\\n",
    "            --dataset-name random \\\n",
    "            --num-prompts 32 \\\n",
    "            --random-input-len 100 \\\n",
    "            --random-output-len 100 \\\n",
    "            --ignore-eos \\\n",
    "            --percentile-metrics ttft,tpot,itl,e2el\n",
    "```\n",
    "\n",
    "Watch the prompts' progress in your previous terminal, where you started the server!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9879f97c",
   "metadata": {},
   "source": [
    "### Play with the server!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b3e4f8",
   "metadata": {},
   "source": [
    "Here's an example of how you can interact with the server through the API. Experiment with it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec28bd",
   "metadata": {},
   "source": [
    "Note: As we did not perform a warm-up here, the first requests will be slow, as they will trigger kernel compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f48155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "from openai import OpenAI\n",
    "tt_base_url = \"http://localhost:8000/v1\"\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c37436",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=tt_base_url,\n",
    "    api_key=\"foobar\" #not used in this example but required to be supplied\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarType(str, Enum):\n",
    "    sedan = \"sedan\"\n",
    "    suv = \"SUV\"\n",
    "    truck = \"Truck\"\n",
    "    coupe = \"Coupe\"\n",
    "\n",
    "\n",
    "class CarDescription(BaseModel):\n",
    "    brand: str\n",
    "    model: str\n",
    "    car_type: CarType\n",
    "\n",
    "\n",
    "json_schema = CarDescription.model_json_schema()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Generate a JSON with the brand, model and car_type of the second most iconic car from the 90's\",\n",
    "        }\n",
    "    ],\n",
    "    extra_body={\"guided_json\": json_schema},\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,\n",
    ")\n",
    "\n",
    "#print(completion.choices[0].text)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd1c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hi! How are you doing?\",\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572674d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed40aea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env_vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
