{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b1e972",
   "metadata": {},
   "source": [
    "# TTNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4bf7e8",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "You don’t need to be proficient in C++ or think about circular buffers to program Tenstorrent devices, just like you don’t need to know CUDA to use PyTorch, or C to use numpy!\n",
    "\n",
    "Meet ttnn - a library for tensor manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0d3074",
   "metadata": {},
   "source": [
    "### Source\n",
    "\n",
    "Like the rest of the Tenstorrent stack, it is open source!\n",
    "\n",
    "You can find the code [here](https://github.com/tenstorrent/tt-metal/tree/main/ttnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91f9bc",
   "metadata": {},
   "source": [
    "### Docs\n",
    "You can also find more documentation and API reference [here](https://docs.tenstorrent.com/tt-metal/latest/ttnn/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ca195b",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26927a0c",
   "metadata": {},
   "source": [
    "TTNN's internals are implemented in C++, but the main way to interact with it is from Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f9dc3e",
   "metadata": {},
   "source": [
    "The library is already installed in your environments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a4d40",
   "metadata": {},
   "source": [
    "Start by opening the Tenstorrent device you will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ttnn.open_device(device_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f649c",
   "metadata": {},
   "source": [
    "### Tensor creation and movement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6e92b",
   "metadata": {},
   "source": [
    "You can create a tensor on host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_tensor = ttnn.full([10,15], 1.0)\n",
    "host_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe3ac7",
   "metadata": {},
   "source": [
    "then move it to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075425e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_tensor = ttnn.to_device(host_tensor, device)\n",
    "device_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb2cf13",
   "metadata": {},
   "source": [
    "or directly create the tensor on device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_tensor_2 = ttnn.rand([10,15], device=device)\n",
    "device_tensor_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f667726",
   "metadata": {},
   "source": [
    "You can also create a ttnn tensor from a torch tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea36f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237fdb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor = torch.rand([10, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad0a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_ttnn_from_torch = ttnn.from_torch(torch_tensor)\n",
    "host_ttnn_from_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad97001",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_ttnn_from_torch = ttnn.from_torch(torch_tensor, device=device, layout=ttnn.TILE_LAYOUT)\n",
    "device_ttnn_from_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21967f23",
   "metadata": {},
   "source": [
    "Sending tensors back is also just as easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac32eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_tensor = ttnn.rand([10,15], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaff5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_tensor = ttnn.from_device(device_tensor)\n",
    "host_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0775d4fb",
   "metadata": {},
   "source": [
    "And moving tensors back to torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe7c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor = ttnn.to_torch(device_tensor)\n",
    "torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca42b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(host_tensor.shape)\n",
    "print(host_tensor.layout)\n",
    "print(host_tensor.dtype)\n",
    "print(host_tensor.memory_config())\n",
    "print(host_tensor.device())\n",
    "print(device_tensor.device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c4c0b",
   "metadata": {},
   "source": [
    "## Tensor layout\n",
    "\n",
    "[Docs about layouts](https://docs.tenstorrent.com/tt-metal/latest/ttnn/ttnn/tensor.html#layout)\n",
    "\n",
    "ttnn.ROW_MAJOR_LAYOUT\n",
    "\n",
    "![](./media/tensor_with_row_major_layout.png)\n",
    "\n",
    "ttnn.TILE_LAYOUT\n",
    "\n",
    "![](./media/tensor_with_tile_layout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633de8c",
   "metadata": {},
   "source": [
    "As you may remember from earlier sections, Tenstorrent devices operate efficiently on tiled data.\n",
    "\n",
    "Many operations require the inputs to be tilized, not row-major. \n",
    "\n",
    "You can change the layout, or choose it when creating or moving the tensor.\n",
    "\n",
    "The tensor gets padded to fill the tiles, but this is transparent to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5978fcd1",
   "metadata": {},
   "source": [
    "Tensors are usually created in row-major layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f68af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttnn.full([3,4], 1.0).layout)\n",
    "print(ttnn.full([3,4], 1.0, device=device).layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a748d5",
   "metadata": {},
   "source": [
    "except in some cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.rand([10,15], device=device).layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feadb10",
   "metadata": {},
   "source": [
    "and maintain their layout when moved to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c6d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_tensor = ttnn.full([3,4], 1.0)\n",
    "print(host_tensor.layout)\n",
    "device_tensor = ttnn.to_device(host_tensor, device)\n",
    "print(device_tensor.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53593d6",
   "metadata": {},
   "source": [
    "unless explicitly converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b5d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_tensor = ttnn.to_layout(device_tensor, ttnn.TILE_LAYOUT)\n",
    "print(device_tensor.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e28a8a",
   "metadata": {},
   "source": [
    "Torch tensors are row-major, but you can tilize during the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor = torch.rand([10,15])\n",
    "print(ttnn.from_torch(torch_tensor).layout)\n",
    "print(ttnn.from_torch(torch_tensor, device=device).layout)\n",
    "print(ttnn.from_torch(torch_tensor, device=device, layout=ttnn.TILE_LAYOUT).layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a4f4c5",
   "metadata": {},
   "source": [
    "## Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e1caa",
   "metadata": {},
   "source": [
    "Supported data types:\n",
    "- uint16\n",
    "- uint32\n",
    "- float32\n",
    "- bfloat16\n",
    "- bfloat8_b\n",
    "- bfloat4_b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ac994",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bf16 = ttnn.rand([1000, 1000], device=device, dtype=ttnn.bfloat16)\n",
    "print(x_bf16)\n",
    "x_uint16 = ttnn.typecast(x_bf16, ttnn.uint16)\n",
    "print(x_uint16)\n",
    "\n",
    "x_bf8_b = ttnn.typecast(x_bf16, ttnn.bfloat8_b)\n",
    "print(x_bf8_b)\n",
    "\n",
    "x_bf4_b = ttnn.typecast(x_bf16, ttnn.bfloat4_b)\n",
    "print(x_bf4_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54ec96",
   "metadata": {},
   "source": [
    "![](./media/bf16.png)\n",
    "![](./media/bf8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bba8c4",
   "metadata": {},
   "source": [
    "### Tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644331ab",
   "metadata": {},
   "source": [
    "_Note_:\n",
    "\n",
    "Most operations are only supported on device, not on host."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0477d54b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To find out more about controlling operation math fidelity and limitations, such as TF32-like matrix multiplication of FP32 inputs, see [details](https://github.com/tenstorrent/tt-metal/blob/main/tech_reports/matrix_engine/matrix_engine.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46468863",
   "metadata": {},
   "source": [
    "Many operations you know and love from pytorch are already here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeea33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ttnn.arange(start=0, end=100, device=device, layout=ttnn.TILE_LAYOUT)\n",
    "x = ttnn.divide(x, 100)\n",
    "x = x.reshape([1,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea2035",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=ttnn.rand([1, 100], device=device)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b471ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x+y\n",
    "x*y\n",
    "x-y\n",
    "ttnn.divide(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c2063",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.sin(x)\n",
    "ttnn.cos(x)\n",
    "ttnn.exp(x)\n",
    "ttnn.log(x)\n",
    "ttnn.sqrt(x)\n",
    "ttnn.pow(x, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955aa0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.sort(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd58bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.concat([x, y], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024cc364",
   "metadata": {},
   "source": [
    "Slicing also works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, 50:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ecb8a",
   "metadata": {},
   "source": [
    "And many, many more!\n",
    "\n",
    "You can find the full set of supported operations [here](https://docs.tenstorrent.com/tt-metal/latest/ttnn/ttnn/api.html#operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889cbc65",
   "metadata": {},
   "source": [
    "## Compilation and compilation cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f488bc6f",
   "metadata": {},
   "source": [
    "You may notice that the operations seem slow when you first run them, and fast the following times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "x = ttnn.rand([1000, 1000], device=device)\n",
    "start = time.time()\n",
    "y = ttnn.softmax(x, dim=1)\n",
    "# because we're not reading back the result,\n",
    "# we need to synchronize to actually measure the execution time,\n",
    "# not just the time taken to dispatch the operation\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"First iteration: {end - start} seconds\")\n",
    "start = time.time()\n",
    "y = ttnn.softmax(x, dim=1)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0524b1a5",
   "metadata": {},
   "source": [
    "This is because when you first run an operation for a given tensor shape, the underlying tt-metal kernel gets compiled. The following runs re-use the same binary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c52f22f",
   "metadata": {},
   "source": [
    "If a compile-time argument changes, such as tensor shape, a new compilation is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a71e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same operation, different shape\n",
    "x = ttnn.rand([1337, 1337], device=device)\n",
    "start = time.time()\n",
    "y = ttnn.softmax(x, dim=1)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"First iteration: {end - start} seconds\")\n",
    "start = time.time()\n",
    "y = ttnn.softmax(x, dim=1)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0669b82",
   "metadata": {},
   "source": [
    "## Direct SRAM (L1) control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d28a9c",
   "metadata": {},
   "source": [
    "As explained in previous sections, with tt-metal and tt-nn the user has full control over moving the data into and out of faster, but limited SRAM memory, also known as L1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5484d94",
   "metadata": {},
   "source": [
    "Size of SRAM:\n",
    "\n",
    "Wormhole (n150): 108MB\n",
    "\n",
    "Wormhole (n300): 192MB\n",
    "\n",
    "Blackhole (p100a): 180MB\n",
    "\n",
    "Blackhole (p150a): 210MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ee381",
   "metadata": {},
   "outputs": [],
   "source": [
    "dram_tensor = ttnn.rand([4096,4096], device=device)\n",
    "dram_tensor.memory_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb902ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sram_tensor = ttnn.to_memory_config(dram_tensor, ttnn.L1_MEMORY_CONFIG)\n",
    "sram_tensor.memory_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warmup, compilation\n",
    "ttnn.sum(dram_tensor, dim = 0)\n",
    "ttnn.sum(sram_tensor, dim = 0)\n",
    "ttnn.synchronize_device(device)\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    ttnn.sum(dram_tensor, dim = 0)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"DRAM Time taken: {end - start} seconds\")\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    ttnn.sum(sram_tensor, dim = 0)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"SRAM Time taken: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af21707",
   "metadata": {},
   "source": [
    "When doing a series of operations, deallocate tensors manually to free up memory. This is especially important for the limited L1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.deallocate(sram_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de803470",
   "metadata": {},
   "source": [
    "For even better performance, you can shard the L1 tensor to keep the data closer to the cores processing it - learn more [here](https://docs.tenstorrent.com/tt-metal/latest/ttnn/ttnn/tensor.html#tensor-sharding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb08d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "width = 500\n",
    "print(\"NoC:\")\n",
    "display(Image(filename='./media/noc.png', width=width))\n",
    "\n",
    "print(\"Interleaved tensor:\")\n",
    "display(Image(filename='./media/interleaved_rm.png', width=width))\n",
    "\n",
    "print(\"Width sharded tensor:\")\n",
    "display(Image(filename='./media/width_sharding.png', width=width))\n",
    "\n",
    "print(\"Height sharded tensor:\")\n",
    "display(Image(filename='./media/height_sharding.png', width=width))\n",
    "\n",
    "print(\"Block sharded tensor:\")\n",
    "display(Image(filename='./media/block_sharding.png', width=width))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a19b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharded_config = ttnn.create_sharded_memory_config(\n",
    "    shape=dram_tensor.shape,\n",
    "    core_grid=ttnn.CoreGrid(x=8, y=8),\n",
    "    strategy=ttnn.ShardStrategy.WIDTH,\n",
    ")\n",
    "\n",
    "sharded_tensor = ttnn.to_memory_config(dram_tensor, sharded_config)\n",
    "ttnn.sum(sharded_tensor, dim=0)\n",
    "ttnn.synchronize_device(device)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    res = ttnn.sum(sharded_tensor, dim=0)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Width sharded Time taken: {end - start} seconds\")\n",
    "ttnn.deallocate(sharded_tensor)\n",
    "\n",
    "sharded_config = ttnn.create_sharded_memory_config(\n",
    "    shape=dram_tensor.shape,\n",
    "    core_grid=ttnn.CoreGrid(x=8, y=8),\n",
    "    strategy=ttnn.ShardStrategy.HEIGHT,\n",
    ")\n",
    "sharded_tensor = ttnn.to_memory_config(dram_tensor, sharded_config)\n",
    "ttnn.sum(sharded_tensor, dim=0)\n",
    "ttnn.synchronize_device(device)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    res = ttnn.sum(sharded_tensor, dim=0)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Height sharded Time taken: {end - start} seconds\")\n",
    "ttnn.deallocate(sharded_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4825bb2",
   "metadata": {},
   "source": [
    "Manual control over L1 lets you keep intermediate results in the cache without fusing operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ttnn.rand([32, 128], device=device, memory_config=ttnn.L1_MEMORY_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb36fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = ttnn.rand([128, 128], device=device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "w2 = ttnn.rand([128, 128], device=device, memory_config=ttnn.L1_MEMORY_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0360ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = ttnn.linear(x, w1, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "print(x1.memory_config())\n",
    "x2 = ttnn.relu(x1) # automatically maintains L1 config\n",
    "print(x2.memory_config())\n",
    "x3 = ttnn.linear(x2, w2, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "print(x3.memory_config())\n",
    "\n",
    "ttnn.deallocate(x1)\n",
    "ttnn.deallocate(x2)\n",
    "ttnn.deallocate(x3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c5df8",
   "metadata": {},
   "source": [
    "### TTNN neural network operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a2b6f5",
   "metadata": {},
   "source": [
    "TTNN provides neural network operations as pure functions, similar to `torch.nn.functional`. This lets you structure your neural network module classes however you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b037c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = ttnn.from_torch(torch.randint(0, 1000, (2, 32)), dtype=ttnn.uint32, device=device)\n",
    "emb_weight = ttnn.rand((1, 1, 1000, 512), dtype=ttnn.bfloat16, device=device)\n",
    "x = ttnn.embedding(input_ids, emb_weight, layout=ttnn.TILE_LAYOUT)  # [2, 32, 512]\n",
    "x = ttnn.reshape(x, (2, 1, 32, 512))\n",
    "# LayerNorm\n",
    "x = ttnn.layer_norm(x, epsilon=1e-5)\n",
    "# Linear: 512 -> 2048 -> 512\n",
    "w1 = ttnn.rand((1, 1, 512, 2048), dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "x = ttnn.relu(ttnn.linear(x, w1))\n",
    "w2 = ttnn.rand((1, 1, 2048, 512), dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "x = ttnn.linear(x, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb12d0",
   "metadata": {},
   "source": [
    "For more operations, like an efficient SDPA implementation, see [here](https://docs.tenstorrent.com/tt-metal/latest/ttnn/ttnn/api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f81bd8",
   "metadata": {},
   "source": [
    "## Inference only\n",
    "\n",
    "You may notice we did not mention autograd - TTNN is focused on inference.\n",
    "\n",
    "Support for training is being developed in a separate framework - have you seen our talk?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b5ca5",
   "metadata": {},
   "source": [
    "## Metal trace\n",
    "\n",
    "Allows to trace all the operations and then repeat them again: \n",
    "\n",
    "```python\n",
    "tid = ttnn.begin_trace_capture(device, cq_id=0)  \n",
    "output = run_model(input)  \n",
    "ttnn.end_trace_capture(device, tid, cq_id=0)  \n",
    "ttnn.execute_trace(device, tid, cq_id=0)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09721b7b",
   "metadata": {},
   "source": [
    "## Multi-device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad7759f",
   "metadata": {},
   "source": [
    "You can shard tensors over multiple devices and perform CCL (collective communication) operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2de820",
   "metadata": {},
   "source": [
    "```python\n",
    "# Open a 1x2 MeshDevice  \n",
    "mesh_device = ttnn.open_mesh_device(ttnn.MeshShape(1, 2))  \n",
    "  \n",
    "# Create a torch tensor  \n",
    "torch_tensor = torch.zeros(1, 1, 32, 64)  \n",
    "torch_tensor[..., 0:32] = 1.0  \n",
    "torch_tensor[..., 32:64] = 2.0  \n",
    "  \n",
    "# Shard the tensor across devices along dimension 3  \n",
    "mesh_tensor = ttnn.from_torch(  \n",
    "    torch_tensor,  \n",
    "    layout=ttnn.TILE_LAYOUT,  \n",
    "    device=mesh_device,  \n",
    "    mesh_mapper=ttnn.ShardTensorToMesh(mesh_device, dim=3),  \n",
    ")\n",
    "\n",
    "output_tensor = ttnn.all_gather(mesh_tensor, dim=3, num_links=1)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
