{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b1e972",
   "metadata": {},
   "source": [
    "## TTNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4bf7e8",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "You don’t need to be proficient in C++ or think about circular buffers to program Tenstorrent devices, just like you don’t need to know CUDA to use PyTorch, or C to use numpy!\n",
    "\n",
    "Meet ttnn - a library for tensor manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0d3074",
   "metadata": {},
   "source": [
    "### Source\n",
    "\n",
    "Like the rest of the Tenstorrent stack, it is open source!\n",
    "\n",
    "You can find the code [here](https://github.com/tenstorrent/tt-metal/tree/main/ttnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91f9bc",
   "metadata": {},
   "source": [
    "### Docs\n",
    "You can also find more documentation and API reference [here](https://docs.tenstorrent.com/tt-metal/latest/ttnn/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ca195b",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26927a0c",
   "metadata": {},
   "source": [
    "TTNN's internals are implemented in C++, but the main way to interact with it is from Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f9dc3e",
   "metadata": {},
   "source": [
    "The library is already installed in your environments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a4d40",
   "metadata": {},
   "source": [
    "Start by opening the Tenstorrent device you will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ttnn.open_device(device_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f649c",
   "metadata": {},
   "source": [
    "### Tensor creation and movement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6e92b",
   "metadata": {},
   "source": [
    "You can create a tensor on host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_tensor =ttnn.full([10,15], 1.0)\n",
    "host_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe3ac7",
   "metadata": {},
   "source": [
    "then move it to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075425e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_tensor = ttnn.to_device(host_tensor, device)\n",
    "device_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb2cf13",
   "metadata": {},
   "source": [
    "or directly create the tensor on device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_tensor_2 = ttnn.rand([10,15], device=device)\n",
    "device_tensor_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f667726",
   "metadata": {},
   "source": [
    "You can also create a ttnn tensor from a torch tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea36f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237fdb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor = torch.rand([10,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad0a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_ttnn_from_torch = ttnn.from_torch(torch_tensor)\n",
    "host_ttnn_from_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad97001",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_ttnn_from_torch = ttnn.from_torch(torch_tensor, device=device, layout=ttnn.TILE_LAYOUT)\n",
    "device_ttnn_from_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21967f23",
   "metadata": {},
   "source": [
    "Sending tensors back is also just as easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac32eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_tensor = ttnn.rand([10,15], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaff5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_tensor = ttnn.from_device(device_tensor)\n",
    "host_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0775d4fb",
   "metadata": {},
   "source": [
    "And moving tensors back to torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe7c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor = ttnn.to_torch(device_tensor)\n",
    "torch_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e07ec7",
   "metadata": {},
   "source": [
    "### Tensor layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633de8c",
   "metadata": {},
   "source": [
    "As you may remember from earlier sections, Tenstorrent devices operate efficiently on tiled data.\n",
    "\n",
    "Many operations require the inputs to be tilized, not row-major. \n",
    "\n",
    "You can change the layout, or choose it when creating or moving the tensor.\n",
    "\n",
    "The tensor gets padded to fill the tiles, but this is transparent to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5978fcd1",
   "metadata": {},
   "source": [
    "Tensors are usually created in row-major layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f68af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttnn.full([3,4], 1.0).layout)\n",
    "print(ttnn.full([3,4], 1.0, device=device).layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a748d5",
   "metadata": {},
   "source": [
    "except in some cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.rand([10,15], device=device).layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feadb10",
   "metadata": {},
   "source": [
    "and maintain their layout when moved to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c6d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_tensor = ttnn.full([3,4], 1.0)\n",
    "print(host_tensor.layout)\n",
    "device_tensor = ttnn.to_device(host_tensor, device)\n",
    "print(device_tensor.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53593d6",
   "metadata": {},
   "source": [
    "unless explicitly converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b5d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_tensor = ttnn.to_layout(device_tensor, ttnn.TILE_LAYOUT)\n",
    "print(device_tensor.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e28a8a",
   "metadata": {},
   "source": [
    "Torch tensors are row-major, but you can tilize during the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor = torch.rand([10,15])\n",
    "print(ttnn.from_torch(torch_tensor).layout)\n",
    "print(ttnn.from_torch(torch_tensor, device=device).layout)\n",
    "print(ttnn.from_torch(torch_tensor, device=device, layout=ttnn.TILE_LAYOUT).layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f299ec3",
   "metadata": {},
   "source": [
    "### Tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644331ab",
   "metadata": {},
   "source": [
    "_Note_:\n",
    "\n",
    "Most operations are only supported on device, not on host."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0477d54b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To find out more about controlling operation math fidelity and limitations, such as TF32-like matrix multiplication of FP32 inputs, see [details](https://github.com/tenstorrent/tt-metal/blob/main/tech_reports/matrix_engine/matrix_engine.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46468863",
   "metadata": {},
   "source": [
    "Many operations you know and love from pytorch are already here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeea33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ttnn.arange(start=0, end=100, device=device, layout=ttnn.TILE_LAYOUT)\n",
    "x = ttnn.divide(x, 100)\n",
    "x = x.reshape([1,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea2035",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=ttnn.rand([1, 100], device=device)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b471ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x+y\n",
    "x*y\n",
    "x-y\n",
    "ttnn.divide(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c2063",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.sin(x)\n",
    "ttnn.cos(x)\n",
    "ttnn.exp(x)\n",
    "ttnn.log(x)\n",
    "ttnn.sqrt(x)\n",
    "ttnn.pow(x, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955aa0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.sort(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd58bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.concat([x, y], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024cc364",
   "metadata": {},
   "source": [
    "Slicing also works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, 50:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ecb8a",
   "metadata": {},
   "source": [
    "And many, many more!\n",
    "\n",
    "You can find the full set of supported operations [here](https://docs.tenstorrent.com/tt-metal/latest/ttnn/ttnn/api.html#operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889cbc65",
   "metadata": {},
   "source": [
    "## Compilation and compilation cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f488bc6f",
   "metadata": {},
   "source": [
    "You may notice that the operations seem slow when you first run them, and fast the following times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "x = ttnn.rand([1000, 1000], device=device)\n",
    "start = time.time()\n",
    "y = ttnn.softmax(x, dim=1)\n",
    "# becuse we're not reading back the result,\n",
    "# we need the synchronize to actually measure the execution time,\n",
    "# not just time taken to dispatch the operation\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"First iteration: {end - start} seconds\")\n",
    "start = time.time()\n",
    "y = ttnn.softmax(x, dim=1)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0524b1a5",
   "metadata": {},
   "source": [
    "This is because when you first run an operation for a given tensor shape, the underlying tt-metal kernel gets compiled. The following runs re-use the same binary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c52f22f",
   "metadata": {},
   "source": [
    "If a compile-time argument changes, such as tensor shape, a new compilation is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a71e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same operation, different shape\n",
    "x = ttnn.x = ttnn.rand([1337, 1337], device=device)\n",
    "start = time.time()\n",
    "y = ttnn.softmax(x, dim=1)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"First iteration: {end - start} seconds\")\n",
    "start = time.time()\n",
    "y = ttnn.softmax(x, dim=1)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0669b82",
   "metadata": {},
   "source": [
    "## Direct SRAM (L1) control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d28a9c",
   "metadata": {},
   "source": [
    "As explained in previous sections, with tt-metal and tt-nn the user has full control over moving the data into and out of faster, but limited SRAM memory, also known as L1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ee381",
   "metadata": {},
   "outputs": [],
   "source": [
    "dram_tensor = ttnn.rand([4096,2048], device=device)\n",
    "dram_tensor.memory_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb902ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sram_tensor = ttnn.to_memory_config(dram_tensor, ttnn.L1_MEMORY_CONFIG)\n",
    "sram_tensor.memory_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warmup, compilation\n",
    "ttnn.sum(dram_tensor, dim = 0)\n",
    "ttnn.sum(sram_tensor, dim = 0)\n",
    "ttnn.synchronize_device(device)\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    ttnn.sum(dram_tensor, dim = 0)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"DRAM Time taken: {end - start} seconds\")\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    ttnn.sum(sram_tensor, dim = 0)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"SRAM Time taken: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af21707",
   "metadata": {},
   "source": [
    "When doing a series of operations, deallocate tensors manually to free up memory. This is especially important for the limited L1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.deallocate(sram_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de803470",
   "metadata": {},
   "source": [
    "For even better performance, you can shard the L1 tensor to keep the data closer to the cores processing it - learn more [here](https://docs.tenstorrent.com/tt-metal/latest/ttnn/ttnn/tensor.html#tensor-sharding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a19b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharded_config = ttnn.create_sharded_memory_config(shape=dram_tensor.shape,\n",
    "core_grid=ttnn.CoreGrid(x=8,y=8),\n",
    "strategy=ttnn.ShardStrategy.WIDTH)\n",
    "sharded_tensor = ttnn.to_memory_config(dram_tensor, sharded_config)\n",
    "ttnn.sum(sharded_tensor, dim = 0)\n",
    "ttnn.synchronize_device(device)\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    res = ttnn.sum(sharded_tensor, dim = 0)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"Sharded Time taken: {end - start} seconds\")\n",
    "ttnn.deallocate(sharded_tensor)\n",
    "\n",
    "sharded_config = ttnn.create_sharded_memory_config(shape=dram_tensor.shape,\n",
    "core_grid=ttnn.CoreGrid(x=8,y=8),\n",
    "strategy=ttnn.ShardStrategy.HEIGHT)\n",
    "sharded_tensor = ttnn.to_memory_config(dram_tensor, sharded_config)\n",
    "ttnn.sum(sharded_tensor, dim = 0)\n",
    "ttnn.synchronize_device(device)\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    res = ttnn.sum(sharded_tensor, dim = 0)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"Sharded 2 Time taken: {end - start} seconds\")\n",
    "ttnn.deallocate(sharded_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4825bb2",
   "metadata": {},
   "source": [
    "Manual control over L1 lets you keep intermediate results in the cache without moving fusing operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ttnn.rand([32, 128], device=device, memory_config=ttnn.L1_MEMORY_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb36fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = ttnn.rand([128, 128], device=device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "w2 = ttnn.rand([128, 128], device=device, memory_config=ttnn.L1_MEMORY_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0360ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = ttnn.linear(x, w1, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "print(x1.memory_config())\n",
    "x2 = ttnn.relu(x1) # automatically maintains L1 config\n",
    "print(x2.memory_config())\n",
    "x3 = ttnn.linear(x2, w2, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "print(x3.memory_config())\n",
    "\n",
    "ttnn.deallocate(x1)\n",
    "ttnn.deallocate(x2)\n",
    "ttnn.deallocate(x3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c5df8",
   "metadata": {},
   "source": [
    "### TTNN neural network operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a2b6f5",
   "metadata": {},
   "source": [
    "TTNN provides neural network operations as pure functions, similar to `torch.nn.functional`. This lets you structure your neural network module classes however you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b037c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = ttnn.from_torch(torch.randint(0, 1000, (2, 32)), dtype=ttnn.uint32, device=device)\n",
    "emb_weight = ttnn.rand((1, 1, 1000, 512), dtype=ttnn.bfloat16, device=device)\n",
    "x = ttnn.embedding(input_ids, emb_weight, layout=ttnn.TILE_LAYOUT)  # [2, 32, 512]\n",
    "x = ttnn.reshape(x, (2, 1, 32, 512))\n",
    "# LayerNorm\n",
    "x = ttnn.layer_norm(x, epsilon=1e-5)\n",
    "# Linear: 512 -> 2048 -> 512\n",
    "w1 = ttnn.rand((1, 1, 512, 2048), dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "x = ttnn.relu(ttnn.linear(x, w1))\n",
    "w2 = ttnn.rand((1, 1, 2048, 512), dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "x = ttnn.linear(x, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb12d0",
   "metadata": {},
   "source": [
    "For more operations, like an efficient SDPA implementation, see [here](https://docs.tenstorrent.com/tt-metal/latest/ttnn/ttnn/api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f81bd8",
   "metadata": {},
   "source": [
    "## Inference only\n",
    "\n",
    "You may notice we did not mention autograd - TTNN is focused on inference.\n",
    "\n",
    "Support for training is being developed in a separate framework - have you seen our talk?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
