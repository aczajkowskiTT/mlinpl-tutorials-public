{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b1e972",
   "metadata": {},
   "source": [
    "# TT-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4bf7e8",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "You don’t need to be proficient in C++ or think about circular buffers to program Tenstorrent devices, just like you don’t need to know CUDA to use PyTorch, or C to use numpy!\n",
    "\n",
    "Meet ttnn - a library for tensor manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0d3074",
   "metadata": {},
   "source": [
    "### Source\n",
    "\n",
    "Like the rest of the Tenstorrent stack, it is open source!\n",
    "\n",
    "You can find the code [here](https://github.com/tenstorrent/tt-metal/tree/main/ttnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91f9bc",
   "metadata": {},
   "source": [
    "### Docs\n",
    "You can also find more documentation and API reference [here](https://docs.tenstorrent.com/tt-metal/latest/ttnn/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ca195b",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26927a0c",
   "metadata": {},
   "source": [
    "TTNN's internals are implemented in C++, but the main way to interact with it is from Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f9dc3e",
   "metadata": {},
   "source": [
    "The library is already installed in your environments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a4d40",
   "metadata": {},
   "source": [
    "Start by opening the Tenstorrent device you will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ttnn.open_device(device_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f649c",
   "metadata": {},
   "source": [
    "### Tensor creation and movement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6e92b",
   "metadata": {},
   "source": [
    "You can create a tensor on host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_tensor = ttnn.full([10, 15], 1.0)\n",
    "host_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe3ac7",
   "metadata": {},
   "source": [
    "then move it to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075425e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_tensor = ttnn.to_device(host_tensor, device)\n",
    "device_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb2cf13",
   "metadata": {},
   "source": [
    "or directly create the tensor on device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_tensor_2 = ttnn.rand([10, 15], device=device)\n",
    "device_tensor_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f667726",
   "metadata": {},
   "source": [
    "You can also create a ttnn tensor from a torch tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea36f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237fdb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor = torch.rand([10, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad0a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_ttnn_from_torch = ttnn.from_torch(torch_tensor)\n",
    "host_ttnn_from_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad97001",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_ttnn_from_torch = ttnn.from_torch(torch_tensor, device=device, layout=ttnn.TILE_LAYOUT)\n",
    "device_ttnn_from_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21967f23",
   "metadata": {},
   "source": [
    "Sending tensors back is also just as easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac32eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_tensor = ttnn.rand([10, 15], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaff5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_tensor = ttnn.from_device(device_tensor)\n",
    "# Alternative - device_tensor.cpu()\n",
    "host_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0775d4fb",
   "metadata": {},
   "source": [
    "And moving tensors back to torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe7c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor = ttnn.to_torch(device_tensor)\n",
    "torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca42b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(host_tensor.shape)\n",
    "print(host_tensor.layout)\n",
    "print(host_tensor.dtype)\n",
    "print(host_tensor.memory_config())\n",
    "print(host_tensor.device())\n",
    "print(device_tensor.device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c4c0b",
   "metadata": {},
   "source": [
    "## Tensor layout\n",
    "\n",
    "[Docs about layouts](https://docs.tenstorrent.com/tt-metal/latest/ttnn/ttnn/tensor.html#layout)\n",
    "\n",
    "ttnn.ROW_MAJOR_LAYOUT\n",
    "\n",
    "![](./media/tensor_with_row_major_layout.png)\n",
    "\n",
    "ttnn.TILE_LAYOUT\n",
    "\n",
    "![](./media/tensor_with_tile_layout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633de8c",
   "metadata": {},
   "source": [
    "As you may remember from earlier sections, Tenstorrent devices operate efficiently on tiled data.\n",
    "\n",
    "Many operations require the inputs to be tilized, not row-major. \n",
    "\n",
    "You can change the layout, or choose it when creating or moving the tensor.\n",
    "\n",
    "The tensor gets padded to fill the tiles, but this is transparent to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5978fcd1",
   "metadata": {},
   "source": [
    "Tensors are usually created in row-major layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f68af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttnn.full([3,4], 1.0).layout)\n",
    "print(ttnn.full([3,4], 1.0, device=device).layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a748d5",
   "metadata": {},
   "source": [
    "except in some cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.rand([10,15], device=device).layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feadb10",
   "metadata": {},
   "source": [
    "and maintain their layout when moved to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c6d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_tensor = ttnn.full([3, 4], 1.0)\n",
    "print(host_tensor.layout)\n",
    "device_tensor = ttnn.to_device(host_tensor, device)\n",
    "print(device_tensor.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53593d6",
   "metadata": {},
   "source": [
    "unless explicitly converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b5d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device_tensor.layout)\n",
    "device_tensor = ttnn.to_layout(device_tensor, ttnn.TILE_LAYOUT)\n",
    "print(device_tensor.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e28a8a",
   "metadata": {},
   "source": [
    "Torch tensors are row-major, but you can tilize during the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor = torch.rand([10,15])\n",
    "print(ttnn.from_torch(torch_tensor).layout)\n",
    "print(ttnn.from_torch(torch_tensor, device=device).layout)\n",
    "print(ttnn.from_torch(torch_tensor, device=device, layout=ttnn.TILE_LAYOUT).layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a4f4c5",
   "metadata": {},
   "source": [
    "## Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e1caa",
   "metadata": {},
   "source": [
    "Supported data types:\n",
    "- uint16\n",
    "- uint32\n",
    "- float32\n",
    "- bfloat16\n",
    "- bfloat8_b\n",
    "- bfloat4_b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ac994",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bf16 = ttnn.rand([1000, 1000], device=device, dtype=ttnn.bfloat16)\n",
    "print(x_bf16)\n",
    "\n",
    "x_float32 = ttnn.typecast(x_bf16, ttnn.float32)\n",
    "print(x_float32)\n",
    "\n",
    "x_uint16 = ttnn.typecast(x_bf16, ttnn.uint16)\n",
    "print(x_uint16)\n",
    "\n",
    "x_bf8_b = ttnn.typecast(x_bf16, ttnn.bfloat8_b)\n",
    "print(x_bf8_b)\n",
    "\n",
    "x_bf4_b = ttnn.typecast(x_bf16, ttnn.bfloat4_b)\n",
    "print(x_bf4_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54ec96",
   "metadata": {},
   "source": [
    "![](./media/bf16.png)\n",
    "![](./media/bf8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bba8c4",
   "metadata": {},
   "source": [
    "### Tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644331ab",
   "metadata": {},
   "source": [
    "_Note_:\n",
    "\n",
    "Most operations are only supported on device, not on host."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0477d54b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To find out more about controlling operation math fidelity and limitations, such as TF32-like matrix multiplication of FP32 inputs, see [details](https://github.com/tenstorrent/tt-metal/blob/main/tech_reports/matrix_engine/matrix_engine.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46468863",
   "metadata": {},
   "source": [
    "Many operations you know and love from pytorch are already here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeea33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ttnn.arange(start=0, end=100, device=device, layout=ttnn.TILE_LAYOUT)\n",
    "x = ttnn.divide(x, 100)\n",
    "x = x.reshape([1, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea2035",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ttnn.rand([1, 100], device=device)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b471ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x + y\n",
    "x * y\n",
    "x - y\n",
    "ttnn.divide(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c2063",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.sin(x)\n",
    "ttnn.cos(x)\n",
    "ttnn.exp(x)\n",
    "ttnn.log(x)\n",
    "ttnn.sqrt(x)\n",
    "ttnn.pow(x, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955aa0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.sort(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd58bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.concat([x, y], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024cc364",
   "metadata": {},
   "source": [
    "Slicing also works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, 50:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ecb8a",
   "metadata": {},
   "source": [
    "And many, many more!\n",
    "\n",
    "You can find the full set of supported operations [here](https://docs.tenstorrent.com/tt-metal/latest/ttnn/ttnn/api.html#operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3d5780",
   "metadata": {},
   "source": [
    "### TTNN neural network operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a2bbf7",
   "metadata": {},
   "source": [
    "TTNN provides neural network operations as pure functions, similar to `torch.nn.functional`. This lets you structure your neural network module classes however you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = ttnn.from_torch(\n",
    "    torch.randint(0, 1000, (2, 32)), dtype=ttnn.uint32, device=device\n",
    ")\n",
    "emb_weight = ttnn.rand((1, 1, 1000, 512), dtype=ttnn.bfloat16, device=device)\n",
    "\n",
    "x = ttnn.embedding(input_ids, emb_weight, layout=ttnn.TILE_LAYOUT)  # [2, 32, 512]\n",
    "x = ttnn.reshape(x, (2, 1, 32, 512))\n",
    "\n",
    "# LayerNorm\n",
    "x = ttnn.layer_norm(x, epsilon=1e-5)\n",
    "\n",
    "# Linear: 512 -> 2048 -> 512\n",
    "w1 = ttnn.rand(\n",
    "    (1, 1, 512, 2048), dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device\n",
    ")\n",
    "x = ttnn.relu(ttnn.linear(x, w1))\n",
    "w2 = ttnn.rand(\n",
    "    (1, 1, 2048, 512), dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device\n",
    ")\n",
    "x = ttnn.linear(x, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e08ea6",
   "metadata": {},
   "source": [
    "Let's explore some commonly used neural network operations in more detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd3decf",
   "metadata": {},
   "source": [
    "For a complete list of all available operations, see the [TTNN API documentation](https://docs.tenstorrent.com/tt-metal/latest/ttnn/ttnn/api.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889cbc65",
   "metadata": {},
   "source": [
    "## Compilation and compilation cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f488bc6f",
   "metadata": {},
   "source": [
    "You may notice that the operations seem slow when you first run them, and fast the following times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "x = ttnn.rand([1000, 1000], device=device)\n",
    "start = time.time()\n",
    "y = ttnn.softmax(x, dim=1)\n",
    "# because we're not reading back the result,\n",
    "# we need to synchronize to actually measure the execution time,\n",
    "# not just the time taken to dispatch the operation\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"First iteration: {end - start} seconds\")\n",
    "start = time.time()\n",
    "y = ttnn.softmax(x, dim=1)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0524b1a5",
   "metadata": {},
   "source": [
    "This is because when you first run an operation for a given tensor shape, the underlying tt-metal kernel gets compiled. The following runs re-use the same binary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c52f22f",
   "metadata": {},
   "source": [
    "If a compile-time argument changes, such as tensor shape, a new compilation is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a71e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same operation, different shape\n",
    "x = ttnn.rand([1337, 1337], device=device)\n",
    "start = time.time()\n",
    "y = ttnn.softmax(x, dim=1)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"First iteration: {end - start} seconds\")\n",
    "start = time.time()\n",
    "y = ttnn.softmax(x, dim=1)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0669b82",
   "metadata": {},
   "source": [
    "## Direct SRAM (L1) control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d28a9c",
   "metadata": {},
   "source": [
    "As explained in previous sections, with tt-metal and tt-nn the user has full control over moving the data into and out of faster, but limited SRAM memory, also known as L1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5484d94",
   "metadata": {},
   "source": [
    "Size of SRAM:\n",
    "\n",
    "Wormhole (n150): 108MB\n",
    "\n",
    "Wormhole (n300): 192MB\n",
    "\n",
    "Blackhole (p100a): 180MB\n",
    "\n",
    "Blackhole (p150a): 210MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ee381",
   "metadata": {},
   "outputs": [],
   "source": [
    "dram_tensor = ttnn.rand([4096, 4096], device=device)\n",
    "dram_tensor.memory_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb902ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sram_tensor = ttnn.to_memory_config(dram_tensor, ttnn.L1_MEMORY_CONFIG)\n",
    "sram_tensor.memory_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warmup, compilation\n",
    "ttnn.sum(dram_tensor, dim=0)\n",
    "ttnn.sum(sram_tensor, dim=0)\n",
    "ttnn.synchronize_device(device)\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    ttnn.sum(dram_tensor, dim=0)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"DRAM Time taken: {end - start} seconds\")\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    ttnn.sum(sram_tensor, dim=0)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "print(f\"SRAM Time taken: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af21707",
   "metadata": {},
   "source": [
    "When doing a series of operations, deallocate tensors manually to free up memory. This is especially important for the limited L1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.deallocate(sram_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de803470",
   "metadata": {},
   "source": [
    "For even better performance, you can shard the L1 tensor to keep the data closer to the cores processing it - learn more [here](https://docs.tenstorrent.com/tt-metal/latest/ttnn/ttnn/tensor.html#tensor-sharding) and [here](https://github.com/tenstorrent/tt-metal/blob/main/tech_reports/tensor_sharding/tensor_sharding.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb08d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "width = 500\n",
    "print(\"NoC:\")\n",
    "display(Image(filename='./media/noc.png', width=width))\n",
    "\n",
    "print(\"Interleaved tensor:\")\n",
    "display(Image(filename='./media/interleaved_rm.png', width=width))\n",
    "\n",
    "print(\"Width sharded tensor:\")\n",
    "display(Image(filename='./media/width_sharding.png', width=width))\n",
    "\n",
    "print(\"Height sharded tensor:\")\n",
    "display(Image(filename='./media/height_sharding.png', width=width))\n",
    "\n",
    "print(\"Block sharded tensor:\")\n",
    "display(Image(filename='./media/block_sharding.png', width=width))\n",
    "\n",
    "print(\"ND Sharded tensor:\")\n",
    "display(Image(filename='./media/nd_sharding_0.png', width=width))\n",
    "display(Image(filename='./media/nd_sharding_1.png', width=width))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a19b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharded_tensor = ttnn.to_memory_config(dram_tensor, ttnn.L1_MEMORY_CONFIG)\n",
    "ttnn.sum(sharded_tensor, dim=0)\n",
    "ttnn.synchronize_device(device)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    res = ttnn.sum(sharded_tensor, dim=0)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "\n",
    "interleaved_l1_time = end - start\n",
    "print(f\"Interleaved L1 Time taken: {interleaved_l1_time * 1000} ms\")\n",
    "ttnn.deallocate(sharded_tensor)\n",
    "\n",
    "sharded_config = ttnn.create_sharded_memory_config(\n",
    "    shape=dram_tensor.shape,\n",
    "    core_grid=ttnn.CoreGrid(x=8, y=8),\n",
    "    strategy=ttnn.ShardStrategy.WIDTH,\n",
    ")\n",
    "\n",
    "sharded_tensor = ttnn.to_memory_config(dram_tensor, sharded_config)\n",
    "ttnn.sum(sharded_tensor, dim=0)\n",
    "ttnn.synchronize_device(device)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    res = ttnn.sum(sharded_tensor, dim=0)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "\n",
    "width_sharded_time = end - start\n",
    "print(f\"Width sharded Time taken: {width_sharded_time * 1000} ms\")\n",
    "ttnn.deallocate(sharded_tensor)\n",
    "\n",
    "sharded_config = ttnn.create_sharded_memory_config(\n",
    "    shape=dram_tensor.shape,\n",
    "    core_grid=ttnn.CoreGrid(x=8, y=8),\n",
    "    strategy=ttnn.ShardStrategy.HEIGHT,\n",
    ")\n",
    "sharded_tensor = ttnn.to_memory_config(dram_tensor, sharded_config)\n",
    "ttnn.sum(sharded_tensor, dim=0)\n",
    "ttnn.synchronize_device(device)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    res = ttnn.sum(sharded_tensor, dim=0)\n",
    "ttnn.synchronize_device(device)\n",
    "end = time.time()\n",
    "\n",
    "height_sharded_time = end - start\n",
    "print(f\"Height sharded Time taken: {height_sharded_time * 1000} ms\")\n",
    "ttnn.deallocate(sharded_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4825bb2",
   "metadata": {},
   "source": [
    "Manual control over L1 lets you keep intermediate results in the cache without fusing operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ttnn.rand([32, 128], device=device, memory_config=ttnn.L1_MEMORY_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb36fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = ttnn.rand([128, 128], device=device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "w2 = ttnn.rand([128, 128], device=device, memory_config=ttnn.L1_MEMORY_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0360ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = ttnn.linear(x, w1, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "print(x1.memory_config())\n",
    "x2 = ttnn.relu(x1) # automatically maintains L1 config\n",
    "print(x2.memory_config())\n",
    "x3 = ttnn.linear(x2, w2, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "print(x3.memory_config())\n",
    "\n",
    "ttnn.deallocate(x1)\n",
    "ttnn.deallocate(x2)\n",
    "ttnn.deallocate(x3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f81bd8",
   "metadata": {},
   "source": [
    "## Inference only\n",
    "\n",
    "You may notice we did not mention autograd - TT-NN is focused on inference.\n",
    "\n",
    "Support for training is being developed in a separate framework - have you seen our talk?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e3b68",
   "metadata": {},
   "source": [
    "## Tooling\n",
    "\n",
    "TT-NN also includes a lot of convenient dev tools, like\n",
    "- [ttnn-visualizer](https://docs.tenstorrent.com/tt-metal/latest/ttnn/ttnn/tutorials/2025_dx_rework/ttnn_visualizer.html)\n",
    "- Host and Device [profiling with tracy](https://docs.tenstorrent.com/tt-metal/latest/ttnn/ttnn/profiling_ttnn_operations.html)\n",
    "- [TT-NN Graph Trace](https://github.com/tenstorrent/tt-metal/blob/main/tech_reports/ttnn/graph-tracing.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf352d1c",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8398353",
   "metadata": {},
   "source": [
    "## Task: implement composite SDPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf155f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# SDPA(Q, K, V) = softmax((Q × K^T) / √d_k) × V\n",
    "def composite_sdpa(q, k, v, causal_mask, scale=None):\n",
    "    # TODO: implement this :)\n",
    "    q_scaled = ...\n",
    "    k_t = ttnn.permute(k, (0, 1, 3, 2))\n",
    "    attn_scores = ...\n",
    "    output = ...\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a13578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "batch, num_heads, seq_len, head_dim = 1, 32, 1024, 128\n",
    "num_iterations, warmup_iterations = 50, 1\n",
    "\n",
    "print(f\"Config: B={batch}, H={num_heads}, S={seq_len}, D={head_dim}, Causal=True\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "Q_torch = torch.randn(batch, num_heads, seq_len, head_dim)\n",
    "K_torch = torch.randn(batch, num_heads, seq_len, head_dim)\n",
    "V_torch = torch.randn(batch, num_heads, seq_len, head_dim)\n",
    "\n",
    "Q_tt = ttnn.from_torch(Q_torch, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "K_tt = ttnn.from_torch(K_torch, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "V_tt = ttnn.from_torch(V_torch, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).unsqueeze(0).unsqueeze(0)\n",
    "causal_mask_tt = ttnn.from_torch(causal_mask, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "\n",
    "print(\"\\n=== Accuracy Test ===\")\n",
    "output_composite = composite_sdpa(Q_tt, K_tt, V_tt, causal_mask_tt)\n",
    "output_composite_torch = ttnn.to_torch(output_composite)[:, :, :seq_len, :head_dim]\n",
    "\n",
    "output_optimized = ttnn.transformer.scaled_dot_product_attention(Q_tt, K_tt, V_tt, is_causal=True)\n",
    "output_optimized_torch = ttnn.to_torch(output_optimized)[:, :, :seq_len, :head_dim]\n",
    "\n",
    "output_torch = torch.nn.functional.scaled_dot_product_attention(Q_torch, K_torch, V_torch, is_causal=True)\n",
    "\n",
    "pcc_composite = torch.corrcoef(torch.stack([output_composite_torch.flatten(), output_torch.flatten()]))[0, 1].item()\n",
    "pcc_optimized = torch.corrcoef(torch.stack([output_optimized_torch.flatten(), output_torch.flatten()]))[0, 1].item()\n",
    "rmse_composite = torch.sqrt(((output_composite_torch - output_torch) ** 2).mean()).item()\n",
    "rmse_optimized = torch.sqrt(((output_optimized_torch - output_torch) ** 2).mean()).item()\n",
    "\n",
    "print(f\"Composite vs PyTorch:  PCC={pcc_composite:.6f}, RMSE={rmse_composite:.6f}\")\n",
    "print(f\"Optimized vs PyTorch:  PCC={pcc_optimized:.6f}, RMSE={rmse_optimized:.6f}\")\n",
    "print(\"\\n=== Speed Test ===\")\n",
    "print(\"Warming up (compiling kernels)...\")\n",
    "for _ in range(warmup_iterations):\n",
    "    out = composite_sdpa(Q_tt, K_tt, V_tt, causal_mask_tt)\n",
    "    out = ttnn.transformer.scaled_dot_product_attention(Q_tt, K_tt, V_tt, is_causal=True)\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_iterations):\n",
    "    output = composite_sdpa(Q_tt, K_tt, V_tt, causal_mask_tt)\n",
    "ttnn.synchronize_device(device)\n",
    "composite_time = (time.perf_counter() - start) / num_iterations * 1000\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_iterations):\n",
    "    output = ttnn.transformer.scaled_dot_product_attention(Q_tt, K_tt, V_tt, is_causal=True)\n",
    "ttnn.synchronize_device(device)\n",
    "optimized_time = (time.perf_counter() - start) / num_iterations * 1000\n",
    "\n",
    "speedup = composite_time / optimized_time\n",
    "\n",
    "print(f\"Composite SDPA: {composite_time:.3f} ms\")\n",
    "print(f\"Optimized SDPA: {optimized_time:.3f} ms\")\n",
    "print(f\"Speedup:        {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d90a9",
   "metadata": {},
   "source": [
    "## Math fidelity\n",
    "\n",
    "TT-NN allows you to have more control over precision of your computations for some operations. \n",
    "\n",
    "For example matrix engine has Math Fidelity. It allows you lower precision, but improve performance.\n",
    "\n",
    "You can find more details [here](https://github.com/tenstorrent/tt-metal/blob/main/tech_reports/matrix_engine/matrix_engine.md) and [here](https://docs.tenstorrent.com/pybuda/latest/dataformats.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc180acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "M, K, N = 2048, 2048, 2048\n",
    "print(f\"> Matrix dimensions: {M}x{K} @ {K}x{N}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn((M, K), dtype=torch.bfloat16)\n",
    "b = torch.randn((K, N), dtype=torch.bfloat16)\n",
    "reference = torch.matmul(a.float(), b.float()) \n",
    "\n",
    "tt_a = ttnn.from_torch(a, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "tt_b = ttnn.from_torch(b, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(f\"{'Fidelity':<10} {'Time (ms)':<12} {'Mean Error':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Test different fidelities\n",
    "fidelities = [\n",
    "    (ttnn.MathFidelity.LoFi, \"LoFi\"),\n",
    "    (ttnn.MathFidelity.HiFi2, \"HiFi2\"),\n",
    "    (ttnn.MathFidelity.HiFi3, \"HiFi3\"),\n",
    "    (ttnn.MathFidelity.HiFi4, \"HiFi4\"),\n",
    "]\n",
    "\n",
    "for fidelity, name in fidelities:\n",
    "    # Configure compute kernel\n",
    "    # Note: Enable FP32 accumulation for HiFi2/HiFi4 to see accuracy benefits\n",
    "    # With BF16 accumulation and large values, LSB corrections can introduce noise\n",
    "    use_fp32_acc = (fidelity != ttnn.MathFidelity.LoFi)\n",
    "    \n",
    "    config = ttnn.WormholeComputeKernelConfig(\n",
    "        math_fidelity=fidelity,\n",
    "        math_approx_mode=False,\n",
    "        fp32_dest_acc_en=use_fp32_acc,  # FP32 for HiFi2/HiFi4\n",
    "        packer_l1_acc=use_fp32_acc,     # L1 accumulation for better precision\n",
    "    )\n",
    "    \n",
    "    # Warm-up\n",
    "    _ = ttnn.matmul(tt_a, tt_b, compute_kernel_config=config)\n",
    "    \n",
    "    # Time the operation\n",
    "    start = time.time()\n",
    "    for _ in range(50):\n",
    "        result_tt = ttnn.matmul(tt_a, tt_b, compute_kernel_config=config)\n",
    "    ttnn.synchronize_device(device)\n",
    "    elapsed = (time.time() - start) / 50 * 1000  # Convert to ms\n",
    "    \n",
    "    # Get result\n",
    "    result = ttnn.to_torch(result_tt).float()\n",
    "    \n",
    "    # Compute errors and PCC\n",
    "    error = torch.abs(reference - result)\n",
    "    mean_err = error.mean().item()\n",
    "    \n",
    "    print(f\"{name:<10} {elapsed:>10.4f}   {mean_err:>10.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b5ca5",
   "metadata": {},
   "source": [
    "## Metal trace\n",
    "\n",
    "Allows to trace all the operations and then repeat them again: \n",
    "\n",
    "```python\n",
    "tid = ttnn.begin_trace_capture(device, cq_id=0)  \n",
    "output = run_model(input)  \n",
    "ttnn.end_trace_capture(device, tid, cq_id=0)  \n",
    "ttnn.execute_trace(device, tid, cq_id=0)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09721b7b",
   "metadata": {},
   "source": [
    "## Multi-device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad7759f",
   "metadata": {},
   "source": [
    "You can shard tensors over multiple devices and perform CCL (collective communication) operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2de820",
   "metadata": {},
   "source": [
    "```python\n",
    "# Open a 1x2 MeshDevice  \n",
    "mesh_device = ttnn.open_mesh_device(ttnn.MeshShape(1, 2))  \n",
    "  \n",
    "# Create a torch tensor  \n",
    "torch_tensor = torch.zeros(1, 1, 32, 64)  \n",
    "torch_tensor[..., 0:32] = 1.0  \n",
    "torch_tensor[..., 32:64] = 2.0  \n",
    "  \n",
    "# Shard the tensor across devices along dimension 3  \n",
    "mesh_tensor = ttnn.from_torch(  \n",
    "    torch_tensor,  \n",
    "    layout=ttnn.TILE_LAYOUT,  \n",
    "    device=mesh_device,  \n",
    "    mesh_mapper=ttnn.ShardTensorToMesh(mesh_device, dim=3),  \n",
    ")\n",
    "\n",
    "output_tensor = ttnn.all_gather(mesh_tensor, dim=3, num_links=1)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
