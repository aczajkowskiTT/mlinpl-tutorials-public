{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb90c81",
   "metadata": {},
   "source": [
    "# MLP Inference with TT-NN\n",
    "\n",
    "This tutorial demonstrates how to leverage TT-NN with PyTorch for neural network inference tasks. We'll explore two practical applications:\n",
    "\n",
    "1. **Regression**: Training a neural network to approximate the `sin(x)` function\n",
    "2. **Classification**: Applying our knowledge to classify handwritten digits from the MNIST dataset\n",
    "\n",
    "Through these examples, you'll learn how to effectively use TT-NN for tensor operations and accelerated model inference on Tenstorrent hardware.\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "### Required Libraries\n",
    "\n",
    "To run MLP inference on Tenstorrent devices, we'll import several key libraries:\n",
    "\n",
    "- **PyTorch (`torch`)**: Core framework for tensor operations and data handling\n",
    "- **TorchVision (`torchvision`)**: Provides access to the MNIST dataset and image preprocessing utilities\n",
    "- **Matplotlib (`matplotlib`)**: Visualization tool for plotting regression results\n",
    "- **TT-NN**: Tenstorrent's neural network library that enables:\n",
    "  - Hardware-accelerated tensor operations\n",
    "  - Efficient data layout transformations\n",
    "  - Optimized layer computations (linear, ReLU, etc.)\n",
    "- **Loguru (`loguru`)**: Enhanced logging for tracking execution progress and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed1fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt  \n",
    "import ttnn\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04659d2",
   "metadata": {},
   "source": [
    "## Open the Device\n",
    "\n",
    "Create the device to run the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c227c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Tenstorrent device\n",
    "device = ttnn.open_device(device_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dffa87",
   "metadata": {},
   "source": [
    "## Sine Function Regression\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this first task, we'll demonstrate how to:\n",
    "- Load pre-trained model weights\n",
    "- Perform inference using TT-NN hardware acceleration\n",
    "- Validate results against expected outputs\n",
    "\n",
    "We'll use a neural network trained to approximate the sine function - a classic regression problem that showcases the model's ability to learn non-linear patterns.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "The pre-trained weights in `mlp_sin.pt` correspond to a simple 3-layer MLP with the following architecture:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(1, 64)       # Input layer: 1 â†’ 64 neurons\n",
    "        self.relu1 = nn.ReLU()           # Activation function\n",
    "        self.l2 = nn.Linear(64, 64)      # Hidden layer: 64 â†’ 64 neurons\n",
    "        self.relu2 = nn.ReLU()           # Activation function\n",
    "        self.l3 = nn.Linear(64, 1)       # Output layer: 64 â†’ 1 neuron\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "### Training Details\n",
    "\n",
    "This model was trained on input values ranging from `-2Ï€` to `2Ï€`, learning to map each input value to its corresponding sine output.\n",
    "\n",
    "### Implementation Steps\n",
    "\n",
    "1. **Generate test data**: Create input values within the trained range\n",
    "2. **Load weights**: Transfer the pre-trained PyTorch weights to TT-NN layers\n",
    "3. **Run inference**: Execute the model on Tenstorrent hardware\n",
    "4. **Visualize results**: Plot predictions against ground truth to verify accuracy\n",
    "\n",
    "Let's begin by implementing each of these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773acdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data for sin(x) approximation\n",
    "x_test = torch.linspace(-2 * math.pi, 2 * math.pi, 200).unsqueeze(1)\n",
    "\n",
    "# Load pretrained weights for the MLP model\n",
    "sin_weights = torch.load(\"mlp_sin.pt\")\n",
    "sin_weight_1 = ttnn.from_torch(sin_weights[\"l1.weight\"], dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "sin_bias_1 = ttnn.from_torch(sin_weights[\"l1.bias\"], dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "sin_weight_2 = ttnn.from_torch(sin_weights[\"l2.weight\"], dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "sin_bias_2 = ttnn.from_torch(sin_weights[\"l2.bias\"], dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "sin_weight_3 = ttnn.from_torch(sin_weights[\"l3.weight\"], dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "sin_bias_3 = ttnn.from_torch(sin_weights[\"l3.bias\"], dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "logger.info(\"Loaded pretrained weights from mlp_sin.pt\")\n",
    "\n",
    "# Prepare weights and biases for TT-NN linear layers\n",
    "sin_weight_1_final = ttnn.transpose(sin_weight_1, -2, -1)\n",
    "sin_bias_1_final = ttnn.reshape(sin_bias_1, [1, -1])\n",
    "sin_weight_2_final = ttnn.transpose(sin_weight_2, -2, -1)\n",
    "sin_bias_2_final = ttnn.reshape(sin_bias_2, [1, -1])\n",
    "sin_weight_3_final = ttnn.transpose(sin_weight_3, -2, -1)\n",
    "sin_bias_3_final = ttnn.reshape(sin_bias_3, [1, -1])\n",
    "\n",
    "# Run inference on test data using TT-NN\n",
    "y_pred = []\n",
    "\n",
    "for i, x in enumerate(x_test):\n",
    "    if i % 50 == 0:\n",
    "        logger.info(f\"Processing sample {i+1}/{len(x_test)}\")\n",
    "    # Convert input to TT-NN tensor\n",
    "    x_tt = ttnn.from_torch(x.unsqueeze(0), dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "\n",
    "    # Layer 1: Linear + ReLU\n",
    "    out1 = ttnn.linear(x_tt, sin_weight_1_final, bias=sin_bias_1_final)\n",
    "    out1 = ttnn.relu(out1)\n",
    "\n",
    "    # Layer 2: Linear + ReLU\n",
    "    out2 = ttnn.linear(out1, sin_weight_2_final, bias=sin_bias_2_final)\n",
    "    out2 = ttnn.relu(out2)\n",
    "\n",
    "    # Layer 3: Linear (output, no activation)\n",
    "    out3 = ttnn.linear(out2, sin_weight_3_final, bias=sin_bias_3_final)\n",
    "\n",
    "    # Convert TT-NN output back to PyTorch tensor and store prediction\n",
    "    prediction = ttnn.to_torch(out3).float()\n",
    "    y_pred.append(float(prediction.cpu().numpy().flatten()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11c2dd",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance\n",
    "\n",
    "### Visualizing Regression Results\n",
    "\n",
    "We've successfully completed inference on our entire test dataset. Now let's assess the model's accuracy by visualizing how well our predictions match the true sine function values.\n",
    "\n",
    "By plotting both the ground truth (actual sine values) and our model's predictions, we can:\n",
    "- Visually inspect the quality of the approximation\n",
    "- Identify any regions where the model struggles\n",
    "- Confirm that our TT-NN implementation produces accurate results\n",
    "\n",
    "Let's create a comparison plot to evaluate our regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot true sin(x) and MLP predictions\n",
    "plt.plot(x_test.numpy(), torch.sin(x_test).numpy(), label=\"True sin(x)\")\n",
    "plt.plot(x_test.numpy(), y_pred, label=\"MLP Prediction\")\n",
    "plt.legend()\n",
    "plt.title(\"MLP Approximation of sin(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b614f",
   "metadata": {},
   "source": [
    "## MNIST Digit Classification\n",
    "\n",
    "### Overview\n",
    "\n",
    "Now we'll apply our TT-NN knowledge to a more complex task: classifying handwritten digits from the famous MNIST dataset. This demonstrates how TT-NN handles real-world classification problems with image data.\n",
    "\n",
    "### Dataset Preparation\n",
    "\n",
    "The MNIST dataset contains 28Ã—28 grayscale images of handwritten digits (0-9). To use this data with our neural network, we need to:\n",
    "\n",
    "1. **Load the dataset**: Download MNIST images and labels\n",
    "2. **Apply transformations**: \n",
    "   - Convert images to tensors\n",
    "   - Normalize pixel values for optimal neural network performance\n",
    "3. **Create a DataLoader**: Enable efficient batch processing and iteration through the dataset\n",
    "\n",
    "This preprocessing pipeline ensures our data is properly formatted for inference on Tenstorrent hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "testset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2998bc6c",
   "metadata": {},
   "source": [
    "## Load Pretrained MLP Weights\n",
    "\n",
    "Load the pretrained MLP weights from a file. Run the following script `train_and_export_mlp.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5439ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained weights\n",
    "weights = torch.load(\"mlp_mnist_weights.pt\")\n",
    "weight_1 = ttnn.from_torch(weights[\"W1\"], dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "bias_1 = ttnn.from_torch(weights[\"b1\"], dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "weight_2 = ttnn.from_torch(weights[\"W2\"], dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "bias_2 = ttnn.from_torch(weights[\"b2\"], dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "weight_3 = ttnn.from_torch(weights[\"W3\"], dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "bias_3 = ttnn.from_torch(weights[\"b3\"], dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "logger.info(\"Loaded pretrained weights from mlp_mnist_weights.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eab2f5",
   "metadata": {},
   "source": [
    "## Running MNIST Inference\n",
    "\n",
    "### Implementation Overview\n",
    "\n",
    "Now we'll implement the inference pipeline for MNIST digit classification using TT-NN. This section demonstrates how to:\n",
    "\n",
    "- Process image data through our MLP model on Tenstorrent hardware\n",
    "- Track prediction accuracy\n",
    "- Handle data format conversions between PyTorch and TT-NN\n",
    "\n",
    "### Key Steps in the Inference Pipeline\n",
    "\n",
    "1. **Data Preparation**\n",
    "   - Flatten 28Ã—28 images into 784-dimensional vectors\n",
    "   - Convert PyTorch tensors to TT-NN format (bfloat16 precision, TILE_LAYOUT)\n",
    "\n",
    "2. **Model Execution**\n",
    "   - Pass data through three fully connected layers\n",
    "   - Apply ReLU activation after the first two layers\n",
    "   - Generate logits for 10 digit classes (0-9)\n",
    "\n",
    "3. **Weight Handling**\n",
    "   - Transpose weight matrices for TT-NN compatibility\n",
    "   - Reshape bias vectors to match expected dimensions\n",
    "\n",
    "4. **Results Processing**\n",
    "   - Convert outputs back to PyTorch tensors\n",
    "   - Extract predictions using argmax\n",
    "   - Compare with ground truth labels\n",
    "   - Log results and calculate accuracy\n",
    "\n",
    "### Exercise: Complete the Implementation\n",
    "\n",
    "Based on the sine regression example, fill in the `TODO` sections to create a working MNIST classifier. Your implementation should:\n",
    "- Process the first five test samples\n",
    "- Display predicted vs. actual digit values\n",
    "- Report the overall accuracy\n",
    "\n",
    "Let's build the inference loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ec32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Prepare weights and biases for TT-NN linear layers.\n",
    "# Transpose weights to match TT-NN's expected shape.\n",
    "# Reshape biases for broadcasting.\n",
    "weight_1_final = None # TODO: Add your code here\n",
    "\n",
    "for i, (image, label) in enumerate(testloader):\n",
    "    if i >= 5:\n",
    "        break\n",
    "\n",
    "    # Flatten image to 1D vector and convert to float32\n",
    "    image = image.view(1, -1).to(torch.float32)\n",
    "    \n",
    "    # Convert PyTorch tensor to TT-NN tensor (bfloat16, TILE_LAYOUT, device)\n",
    "    image_tt = ttnn.from_torch(image, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "    \n",
    "    # Layer 1: Linear + ReLU\n",
    "    # TODO: Add your code here\n",
    "    \n",
    "    # Layer 2: Linear + ReLU\n",
    "    # TODO: Add your code here\n",
    "    \n",
    "    # Layer 3: Linear (output logits, no activation)\n",
    "    out3 = None # TODO: Add your code here\n",
    "\n",
    "    # Convert TT-NN output back to PyTorch tensor\n",
    "    prediction = ttnn.to_torch(out3)\n",
    "    predicted_label = torch.argmax(prediction, dim=1).item()\n",
    "    \n",
    "    # Update accuracy counters\n",
    "    correct += predicted_label == label.item()\n",
    "    total += 1\n",
    "    \n",
    "    logger.info(f\"Sample {i+1}: Predicted={predicted_label}, Actual={label.item()}\")\n",
    "    \n",
    "logger.info(f\"\\nTT-NN MLP Inference Accuracy: {correct}/{total} = {100.0 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835de03d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "ðŸŽ‰ **Congratulations!** You've successfully completed two fundamental neural network tasks using TT-NN:\n",
    "\n",
    "- **Regression**: Approximating the sine function with high accuracy\n",
    "- **Classification**: Recognizing handwritten digits from the MNIST dataset\n",
    "\n",
    "Through these examples, you've learned how to:\n",
    "- Load pre-trained weights into TT-NN models\n",
    "- Convert data between PyTorch and TT-NN formats\n",
    "- Execute inference on Tenstorrent hardware\n",
    "- Evaluate model performance\n",
    "\n",
    "### Cleaning Up\n",
    "\n",
    "Before finishing, it's important to properly close the Tenstorrent device to release hardware resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.close_device(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
